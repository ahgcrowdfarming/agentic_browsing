name: Monthly Supermarket Scrape & Publish

on:
  schedule:
    # 04:00 UTC on the 1st of every month (â‰ˆ 05:00/06:00 Europe/Madrid)
    - cron: '0 4 1 * *'
  workflow_dispatch: {}

# avoid overlapping monthly runs
concurrency:
  group: supermarket-scrape-publish
  cancel-in-progress: false

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read

    # ðŸ‘‡ make all `run:` commands execute from your nested folder
    defaults:
      run:
        working-directory: examples/browser/supermarket_scrapper_v2

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Show working directory
        run: |
          pwd
          ls -la

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python deps
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install playwright requests pandas
          fi

      - name: Install Playwright browsers & OS deps
        run: python -m playwright install --with-deps chromium

      - name: Prepare CI runtime & profile dir
        # create XDG_RUNTIME_DIR and a writable browser profile location for CI
        run: |
          echo "XDG_RUNTIME_DIR=/tmp/runtime-$GITHUB_RUN_ID" >> $GITHUB_ENV
          mkdir -p "/tmp/runtime-$GITHUB_RUN_ID" || true
          echo "BROWSER_USER_DATA_DIR=$GITHUB_WORKSPACE/.browser_profile" >> $GITHUB_ENV
          mkdir -p "$GITHUB_WORKSPACE/.browser_profile" || true
          echo "Prepared runtime dir and browser profile dir"

      - name: Debug env (sanity check)
        run: |
          echo "XDG_RUNTIME_DIR=$XDG_RUNTIME_DIR"
          echo "BROWSER_USER_DATA_DIR=$BROWSER_USER_DATA_DIR"
          python -V
          pip show playwright || true

      - name: Install Xvfb
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb

      - name: Run pipeline (scrape -> CSV -> Foundry upload)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          FOUNDRY_HOST: ${{ secrets.FOUNDRY_HOST }}
          FOUNDRY_DATASET_RID: ${{ secrets.FOUNDRY_DATASET_RID }}
          FOUNDRY_TOKEN: ${{ secrets.FOUNDRY_TOKEN }}
          FOUNDRY_FOLDER_PREFIX: ${{ vars.FOUNDRY_FOLDER_PREFIX }}
          CSV_FILENAME: ${{ vars.CSV_FILENAME }}
          BROWSER_USER_DATA_DIR: ${{ env.BROWSER_USER_DATA_DIR }}
          # Optional debugging:
          DEBUG: pw:api
          # Optional proxy vars (only set these if you have a proxy; otherwise leave them absent)
          # HTTP_PROXY: ${{ secrets.HTTP_PROXY }}
          # HTTPS_PROXY: ${{ secrets.HTTPS_PROXY }}
          # NO_PROXY: ${{ secrets.NO_PROXY }}
          # PROXY_SERVER: ${{ secrets.PROXY_SERVER }}   # e.g. http://user:pass@host:port
          # BROWSER_USER_AGENT: ${{ vars.BROWSER_USER_AGENT }} # if you want to force a UA
        run: |
          set -euxo pipefail
          export XDG_RUNTIME_DIR="${XDG_RUNTIME_DIR:-/tmp/runtime-$GITHUB_RUN_ID}"
          # Quick network sanity checks:
          echo "== curl head to Carrefour =="
          curl -I --max-time 20 https://www.carrefour.fr || true
          echo "== curl to OpenAI models endpoint =="
          curl -sS --max-time 20 -w "\nHTTP %{http_code}\n" https://api.openai.com/v1/models -H "Authorization: Bearer $OPENAI_API_KEY" -o /dev/null || true
          # Run under a proper virtual display at 1080p:
          xvfb-run -a -s "-screen 0 1920x1080x24" python foundry_publish.py

      - name: Show outputs
        if: always()
        run: |
          echo "Tree of output/"
          ls -la output || true
          # if you still have per-country subfolders, this shows a couple of levels:
          find output -maxdepth 2 -type d -print | sort || true

      - name: Upload last_run.csv as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: last_run_csv
          path: examples/browser/supermarket_scrapper_v2/output/last_run.csv
          if-no-files-found: warn

      - name: Upload full output folder as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-output
          path: examples/browser/supermarket_scrapper_v2/output
          if-no-files-found: warn

      - name: Upload browser profile for debugging
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: browser-profile
          path: ${{ github.workspace }}/.browser_profile
          if-no-files-found: warn

      - name: Upload Playwright traces / logs (if any)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-logs
          path: |
            examples/browser/supermarket_scrapper_v2/playwright-logs
            examples/browser/supermarket_scrapper_v2/*.log
          if-no-files-found: warn